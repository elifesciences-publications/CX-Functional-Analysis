using JLD,DataFrames,AxisArrays
using Interpolations
using StatsBase,Distributions #GLM,
using JSON
using DataStructures
using Distances,Bootstrap

## Load the labbook, the lines description table and the fluorescence data
labbook_table = JLD.load("labbookTable.jld")["df"];
linesToType = readtable("LinesAndTypes.csv");
full_data_dict = JLD.load("rawData.jld");

## An per run average version of the fluorescence data (to make figures)
avg_data_dict = map(full_data_dict) do full  
    (k,full) = full
    meanSignals = [(AxisArray(vec(mean(f[1].data,3)),axes(f[1],Axis{:time})),f[2]) for 
        f in full]
    Pair(k,meanSignals)    
end;
JLD.save("avgData.jld",avg_data_dict)

## Interpolating those averaged runs (useful to compute correlations)
function interpolate_run(run,time_axis)
    itp = interpolate((run[1][Axis{:time}][:],),vec(run[1]),Gridded(Linear()))
    (AxisArray(itp[time_axis],Axis{:time}(time_axis)),run[2])
end
interpolated_data_dict = map(avg_data_dict) do dct
        (k,val) = dct
        newAvg = [interpolate_run(run,3:0.1:5) for run in val]     ## The two seconds following the stimulation
        Pair(k,newAvg)
end;

## A function that returns the stats per repeats (the input is an array of run generated by the raw analysis script) MOVE TO FUNC CONN PACKAGE ?
function getStatistics(runArray)
    nRuns = length(runArray)
    periods = [x[1][Axis{:time}][2] for x in runArray]
    nPulses = [x[2]["pulseNumber"] for x in runArray]
    
    resultsDf = DataFrame() 
    
    for tt in 1:nRuns
        run = squeeze(runArray[tt][1],2)
        ## The times we consider to be "stim" (goes a bit after the stim to account for GC6m rising time and CsChrimson dynamics, important for very short stimulations)
        stimTimes = 3..(3+max(nPulses[tt]/30,periods[tt]))
        
        ## Times used for the calculation of baselines
        preTimes = 1.9..2.9
        ## Second after stim
        postTimes = (3+nPulses[tt]/30)..(4+nPulses[tt]/30)
        fullTimes = 3..(4+nPulses[tt]/30)
        
        baseline = mean(run[preTimes,:],1)
        
        fluoStim = run[stimTimes,:]
        stimStart = fluoStim[Axis{:time}][1]
        fluoFull = run[fullTimes,:]
        
        integrals = vec(sum(fluoStim.-baseline,1))*periods[tt]
        # We want to look at the peak that has the same sign as the integral                    
        extremaFunction = [integrals[i] >= 0 ? maximum : minimum for 
            i in eachindex(integrals)]  
        findFunction = [integrals[i] >= 0 ? indmax : indmin for i in eachindex(integrals)]
            
        peakFluo = vec([extremaFunction[i](fluoFull[:,i]-baseline[i],1)[1] 
                for i in eachindex(extremaFunction)]       )
        peakTimes = [fluoFull[Axis{:time}][findFunction[i](fluoFull[:,i])]-stimStart 
            for i in eachindex(peakFluo)]
        
        ## Find the half-decay time (considering baseline as the value it's coming back to)
        halfDecay = [findfirst(abs.(run[(peakTimes[i]+stimStart)..Inf,i]-baseline[i]).<abs(peakFluo[i]/2))*periods[tt] for i in eachindex(peakTimes)]
        
        ## Integral to the peak of the response
        integralToPeak = vec([sum(run[3..(peakTimes[i]+stimStart),i]-baseline[i],
                    1)[1]*periods[tt] for i in eachindex(peakTimes)])
        
        repeatsCor = repeat([(sum(cor(run[3..3.66,:],1))-4)/12],inner=[4])  
        ## Getting the mean correlation during the 
        ## stimulus during the 4 repeats as a measure 
        ## of reliability (it's the same value repeated here)

        ## Measurements normalized by baseline
        peakFluoNorm = peakFluo./vec(baseline)
        integralToPeakNorm = (integralToPeak - nPulses[tt]*0.001)./vec(baseline) ## Small ad-hoc correction to avoid influence of the stim artefact
        
        miniDf = DataFrame(Any[vec(baseline),
                               integrals,
                               integralToPeak,
                               integralToPeakNorm,
                               peakFluo,peakTimes,peakFluoNorm,
                               repeatsCor,halfDecay,
                               repeat([nPulses[tt]],inner=[4]),
                               repeat([tt],inner=[4])],
                           [:baseline,
                            :integrals,
                            :integral_to_peak,
                            :integNorm,
                            :peakFluo,
                            :peakTimes,
                            :peakNorm,
                            :repeats_correlation,
                            :half_decay,
                            :nPulses,
                            :runIdx])
        
        resultsDf = vcat(resultsDf,miniDf)
    end
    resultsDf
end

## Run the stats computation on a dict of runs and concatenate them in a single data frame
function compileStats(runArrayDict,keys)
    statsDf = DataFrame()
    for (k,runArray) in runArrayDict
        if (k in keys)
            runDf = getStatistics(runArray)
            runDf[:experiment] = k
            statsDf = vcat(statsDf,runDf)
        end
    end
   
    statsDf
end

keyEntries = labbook_table[:keyEntry];
fullStats = compileStats(full_data_dict,keyEntries)

## Calculate the medians of the statistics
stats_per_run = aggregate(fullStats,[:experiment,:runIdx],median)
stats_per_run[:nPulses_median] = convert(Array{Int},stats_per_run[:nPulses_median]);

## Add some metadata columns 
stats_per_run[:cellPair]=""
stats_per_run[:genotype]=""
stats_per_run[:preNeuron]=""
stats_per_run[:postNeuron]=""
stats_per_run[:preDrug]=true
stats_per_run[:Drug]=""
stats_per_run[:timeToDrug]= -Inf
for i in 1:size(stats_per_run,1)
    stats_per_run[i,:cellPair] = labbook_table[findfirst(keyEntries.==stats_per_run[i,:experiment]),:cellToCell]
    stats_per_run[i,:genotype] = labbook_table[findfirst(keyEntries.==stats_per_run[i,:experiment]),:genotypeRegion]
    stats_per_run[i,:preNeuron] = labbook_table[findfirst(keyEntries.==stats_per_run[i,:experiment]),:cellPre]
    stats_per_run[i,:postNeuron] = labbook_table[findfirst(keyEntries.==stats_per_run[i,:experiment]),:cellPost]
    if (length(labbook_table[findfirst(keyEntries.==stats_per_run[i,:experiment]),:Drug])>0)
        stats_per_run[i,:preDrug] = (labbook_table[findfirst(keyEntries.==stats_per_run[i,:experiment]),
            :timesToDrug][stats_per_run[i,:runIdx]])>Dates.Second(0)
        stats_per_run[i,:timeToDrug] = -Int64(Dates.value(labbook_table[findfirst(keyEntries.==stats_per_run[i,:experiment]),
            :timesToDrug][stats_per_run[i,:runIdx]]))/60000
        stats_per_run[i,:Drug]=labbook_table[findfirst(keyEntries.==stats_per_run[i,:experiment]),:Drug]
    end
end

## Calculate the number of runs per pair and select pairs with at least 3 runs
ns = by(stats_per_run,:cellPair,df -> DataFrame(n = length(unique(df[:experiment]))))
fullExps = ns[ns[:n].>2,:cellPair];

stats_per_run = stats_per_run[in.(Array(stats_per_run[:cellPair]),[fullExps]),:]

## List the cell types tested
uniqueTypesUsed = unique(vcat(stats_per_run[:preNeuron],stats_per_run[:postNeuron]))
sort!(uniqueTypesUsed,by= v->linesToType[findfirst(linesToType[:Type_Description].==v),:Supertype] );

## Parse the neuron types to define their pre/post regions
possibleNeuropiles = ["PB","FB","EB","NO","GA","LAL","rub","BU"];
neuronTypes = OrderedDict(uniqueTypesUsed[i] =>
              Dict("innervates" => filter(x -> contains(uniqueTypesUsed[i],x),
                                                                possibleNeuropiles), 
             "pre"=> split(linesToType[findfirst(linesToType[:,:Type_Description].==uniqueTypesUsed[i]),
                :Pre_regions],","),
             "post" => split(linesToType[findfirst(linesToType[:,:Type_Description].==uniqueTypesUsed[i]),
                :Post_regions],","),
             "pre_fine"=> split(linesToType[findfirst(linesToType[:,:Type_Description].==uniqueTypesUsed[i]),
                :Pre_regions_fine],","),
             "post_fine" => split(linesToType[findfirst(linesToType[:,:Type_Description].==uniqueTypesUsed[i]),
                :Post_regions_fine],","),
             "short_name" => linesToType[findfirst(linesToType[:,:Type_Description].==uniqueTypesUsed[i]),
            :New_Type_Name]   
        ) for 
    i in 1:length(uniqueTypesUsed));

## Using the annotation, establish if there's a potential overlap for every pair 
stats_per_run[:overlapping]=
[(length(intersect(neuronTypes[ty]["pre_fine"],
        neuronTypes[tyPost]["post_fine"]))>0) for (ty,tyPost) in zip(stats_per_run[:preNeuron],
                                                                     stats_per_run[:postNeuron])]

## Is the same neuron used for recording and stimulation ?
stats_per_run[:self]= (stats_per_run[:preNeuron].==stats_per_run[:postNeuron]);

stats_per_run[:expType] = "Non overlapping"
stats_per_run[stats_per_run[:overlapping],:expType] = "Overlapping"
stats_per_run[stats_per_run[:self],:expType] = "Self stimulation"
stats_per_run[:overlapping] =  stats_per_run[:expType].=="Overlapping"; ## Excluding self activation

## To compute some metrics, we want to symetrize some response 
## stats around zero (rescale to 
## account for the fact that inhibitory responses are bounded) 
scaleResponse = function(resp)
    getNorm = function(x)
        ret = (x>=0 ? (x/(maximum(resp))):(-x/(minimum(resp))))
        ret
    end
    [getNorm(x) for x in resp]
end

## A function that removes outliers to get a better estimate of the covariance matrix for
## distance evaluation TO COMMIT TO SOME STAT PACKAGE
function fastMCD(X,p = ceil(Int,(sum(size(X))+1)/2);nrepeats=500)
    rng = Base.GLOBAL_RNG
    hMin = nothing
    sMin = Inf
  
    for i in 1:nrepeats
        idx = randperm(size(X,1))
        h1 = X[idx[1:p],:]    
        s0 = 0
        s1= 1
        while ((det(s1)!=det(s0)) & (det(s1)!=0))
            h0 = h1
            s0 = cov(h0)
            m = vec(mean(h0,1))
            Dis = vec(mapslices(x -> mahalanobis(x,m,inv(s0)),X,2))
            ord = sortperm(Dis)
            h1 = X[ord[1:p],:]
            s1=cov(h1)
       end
       
        if det(s1)<det(sMin)
            hMin = h1
            sMin = s1
        end
    end
    ## Reweighting
    sfull = cov(hMin)
    tmcd = vec(mean(hMin,1))
    dfull = vec(mapslices(x -> mahalanobis(x,tmcd,inv(sfull)),hMin,2))
    smcd = (median(dfull.^2)/pdf(Chisq(size(X,2)),0.5))*sfull
    dmcd = vec(mapslices(x -> mahalanobis(x,tmcd,inv(smcd)),hMin,2));
    w = FrequencyWeights(((dmcd.^2).<pdf(Chisq(size(X,2)),0.975))*1)
    t1 = mean(hMin,w,1)
    s1 = cov(hMin,w,corrected=true)
    (t1,s1)
end

    stats_per_run[:integNorm_scaled] =  scaleResponse(stats_per_run[:integNorm_median])
    stats_per_run[:integral_to_peak_scaled] =  scaleResponse(stats_per_run[:integral_to_peak_median])
    ## Distance evaluation, statistical significance
    ## We're going to use those stats for distance 
    stats_to_use = [:integral_to_peak_scaled,
                :repeats_correlation_median]

    ## Per run distance evaluation, evaluated separately for each stim condition
function addDistances!(statDf,stats_to_use,signVariable)
    statDf[:signif1]=false
    statDf[:signif5]=false
    statDf[:distance]=0.0
    for p in [1,5,10,20,30]
        locDF = statDf[statDf[:nPulses_median].==p,:]
        null_sample = locDF[locDF[:expType].=="Non overlapping",stats_to_use]
        ## Robust mean and covariance of the control sample
        (null_mean,null_cov) = fastMCD(convert(Array,null_sample))
    
        ## Distance evaluation
        dM = mapslices(x-> mahalanobis(x,vec(null_mean),inv(null_cov)),
                       convert(Array,locDF[:,stats_to_use]),2)
        
        ## Use the null distribution of distances to evaluate the confidence intervals
        null_dist = dM[locDF[:expType].=="Non overlapping"]
        centerB = bootstrap(null_dist,mean,BalancedSampling(1000))
        s1 = Bootstrap.ci(centerB,BCaConfInt(0.98))[1][3]
        s5 = Bootstrap.ci(centerB,BCaConfInt(0.9))[1][3]
   
        ## Significance calculation, update the table
        statDf[statDf[:nPulses_median].==p,:signif1] = dM.> s1 ;
        statDf[statDf[:nPulses_median].==p,:signif5] = dM.> s5 ;
        statDf[statDf[:nPulses_median].==p,:distance] = dM .* sign.(statDf[statDf[:nPulses_median].==p,
                                                                           signVariable]);
    end
    statDf
end
    
addDistances!(stats_per_run,stats_to_use,:integral_to_peak_median)

## Distance normalized to the maximum value
stats_per_run[:distanceNorm] = stats_per_run[:distance]./maximum(abs(stats_per_run[:distance]));

## Drug experiments
# Selecting mecamylamine runs    
mecadf = stats_per_run[(stats_per_run[:Drug].=="Mecamylamine") .& (stats_per_run[:timeToDrug].>-5),:]

# Same thing for picrotoxin
picrodf = stats_per_run[(stats_per_run[:Drug].=="Picrotoxin") .& (stats_per_run[:timeToDrug].>-5),:]

## A function to aggregate stats for a group of runs (for example for a cell pair) TO MOVE TO FUNC CONN PACKAGE ?
function category_stats(df)
    ## Calculate the between run correlations for a given pair/genotype
    avg_data = vcat([interpolated_data_dict[exp][df[df[:experiment].==exp,:runIdx]] for 
                exp in unique(df[:experiment])]...)
    cc = hcat([a[1] for a in avg_data]...)
    cc = cor(cc,1)
     
    ## THE NORMALIZED VARIABLES ARE CRUDE ESTIMATES, MIGHT BE WORTH SWITCHING TO SOME ROBUST REGRESSION WHEN IT'S MATURE IN JULIA
    
    DataFrame(peakFluo = median(df[:peakFluo_median]),
              peakNorm = median(df[:peakNorm_median]),#median(df[:peakFluo_median]./df[:baseline_median]),#predi["peak"][1],#a + b,#itp[0.3],
              peakNormMAD = mad(df[:peakNorm_median]),#mad(df[:peakFluo_median]./df[:baseline_median]),
              integ = median(df[:integral_to_peak_median]),
              integNorm = median(df[:integNorm_median]),#median(df[:integral_to_peak_median]./df[:baseline_median]),#predi["integral"][1],#a + b,#itp[0.3],
    integNormMAD = mad(df[:integNorm_median]),#mad(df[:integral_to_peak_median]./df[:baseline_median]),
    baseline = median(df[:baseline_median]),
        repeats_corr = median(df[:repeats_correlation_median]),
        peakTime = median(df[:peakTimes_median]),
    state_dependence = cor(df[:distance],df[:baseline_median]),
     state_dependence_peak = cor(df[:peakFluo_median],df[:baseline_median]),
        state_dependence_integral = cor(df[:integral_to_peak_median],df[:baseline_median]),
        between_runs_corr = (sum(cc)-size(cc,1))/(length(cc) - size(cc,1)),
        run_distance = median(df[:distance]),
        preNeuron = df[:preNeuron][1],
        postNeuron = df[:postNeuron][1],
        expType = df[:expType][1],
        decay_time = median(df[:half_decay_median]),
        n = size(df,1)
        ) 
end

## Calculate the stats for each pair
stats_per_pair = by(stats_per_run[stats_per_run[:preDrug],:],[:cellPair,:nPulses_median],category_stats)

function getDrugStats(df)
    DataFrame(
              drugEffect = (median(df[(15.>df[:timeToDrug].>11),:integral_to_peak_median])-median(df[(0.>df[:timeToDrug].>-5),:integral_to_peak_median]))/abs(median(df[(0.>df[:timeToDrug].>-5),:integral_to_peak_median]))
              )
end

drugStatsDF = by(stats_per_run[(stats_per_run[:timeToDrug].>-5),:],[:experiment,:cellPair,:Drug],getDrugStats)

drugStatsDFPerPair = aggregate(drugStatsDF[:,[:cellPair,:Drug,:drugEffect]],[:Drug,:cellPair],[median,mad])
## For each pair compute fit a linear model to the dose response curve
function getPairDoseResponse(df)
    #peakNormLM = lm(@formula(peakNorm ~ nPulses_median),df)
     
    DataFrame(
     dose_peak_norm = cor(df[:nPulses_median],df[:peakNorm]),
     dose_slope_peak_norm = median(df[:peakNorm]./df[:nPulses_median])#coef(peakNormLM)[2]
    )
end

## No 30 pulses for that fit as it seems to saturate
doseRespDF = by(stats_per_pair[stats_per_pair[:nPulses_median].<=20,:],[:cellPair],getPairDoseResponse);

## Scaled responses
stats_per_pair[:integNormScaled] = scaleResponse(stats_per_pair[:integNorm])
#stats_per_pair[:dose_slope_peak_normScaled] = scaleResponse(stats_per_pair[:dose_slope_peak_norm])


stats_to_use2 = [:integNormScaled,
                 :repeats_corr
               ];

addDistances!(stats_per_pair,stats_to_use2,:integNormScaled)
## Add a signed significance to be used in summary diagrams/matrices
stats_per_pair[:globalSignif] =  sign.(stats_per_pair[:distance]).*stats_per_pair[:signif1]
stats_per_pair[stats_per_pair[:globalSignif].==-0.0,:globalSignif]=0
stats_per_pair[:distanceNorm] = stats_per_pair[:distance]./maximum(abs(stats_per_pair[:distance]));
#stats_per_pair_20[:globalSignif] =  sign.(stats_per_pair_20[:distance]).*stats_per_pair_20[:signif1]
#stats_per_pair_20[stats_per_pair_20[:globalSignif].==-0.0,:globalSignif]=0

## Using the 20 pulses significance results for everything here
stats_per_pair_20 = stats_per_pair[stats_per_pair[:nPulses_median].==20,:]
stats_per_pair[:signif20] = stats_per_pair_20[indexin(stats_per_pair[:cellPair],
                                                      stats_per_pair_20[:cellPair]),:globalSignif]
stats_per_pair[stats_per_pair[:globalSignif].==-0.0,:globalSignif]=0
stats_per_pair_20 = join(stats_per_pair_20,doseRespDF,on=:cellPair,kind=:left)

#stats_per_run[:globalSignif] = stats_per_pair_20[indexin(stats_per_run[:cellPair],
#        stats_per_pair_20[:cellPair]),:signif1] .* sign(stats_per_pair_20[indexin(stats_per_run[:cellPair],
#        stats_per_pair_20[:cellPair]),:integNorm]);
#stats_per_run[stats_per_run[:globalSignif].==-0.0,:globalSignif]=0

## Export the stats (to be used by the plotting scripts)
JLD.save("statTables.jld","stats_per_run",stats_per_run,"stats_per_pair",stats_per_pair,"uniqueTypesUsed",uniqueTypesUsed,"stats_per_pair_20",stats_per_pair_20)

JLD.save("drugTables.jld","mecadf",mecadf,"picrodf",picrodf,"drugStats",drugStatsDFPerPair)
## Functions for export to javascript
function writeJS(name,variable_name,someDict)
    out_data = JSON.json(someDict)
    open(name, "w") do f
        write(f, "const ",variable_name,"=",out_data)
    end
end

## Functions to export fluorescence data (we need dicts for js, and we're selecting the first 6 runs (no drug) for now)
function get_dataDict_per_key(pk,data_dict)
    dat = data_dict[pk][1:min(6,end)]
    if length(dat)==0
        return(0)
    else
        dat = Dict(x[2]["pulseNumber"] => transformAxisArray(x[1]) for x in dat)
    end
end

function transformAxisArray{T}(aa::AxisArray{T,1})
    Dict("x"=>axes(aa,1)[:]-axes(aa,1)[findfirst(axes(aa,1)[:].>3.0)],"y"=>aa)
end

transformAxisArray{T}(aa::AxisArray{T,3}) = Dict("x"=>axes(aa,1)[:]-axes(aa,1)[findfirst(axes(aa,1)[:].>3.0)],
                                                 "y"=>reshape(aa,(size(aa,1)*size(aa,2),size(aa,3))))

transformAxisArray{T}(AA::Array{AxisArrays.AxisArray{T,1,Array{T,1},Tuple{AxisArrays.Axis{:time,StepRangeLen{Float64}}}},1}) =  Dict("x"=>[axes(aa,1)[:]-axes(aa,1)[findfirst(axes(aa,1)[:].>3.0)] for aa in AA],"y"=>AA)

## Exporting the full fluorescence and the average
out_data = Dict(pk => get_dataDict_per_key(pk,full_data_dict) for pk in keyEntries)
filter!((x,y) -> y!=0,out_data)
    
writeJS("../full_data.js","FULL_DATA",out_data)

out_data_avg = Dict(pk => get_dataDict_per_key(pk,avg_data_dict) for pk in keyEntries)
filter!((x,y) -> y!=0,out_data_avg)
    
writeJS("../avg_data.js","AVG_DATA",out_data_avg)

## Exporting a table mapping the experiments to the cell pairs
pairToExp = Dict(cpair => convert(Array{String},labbook_table[convert(Array{Bool}
                ,labbook_table[:cellToCell].==cpair),:keyEntry]) for 
        cpair in unique(labbook_table[:cellToCell]))

writeJS("../pairsToExp.js","PAIRS_TO_EXP",pairToExp)

supertypes = unique(linesToType[:Supertype])

## Exporting a supertype description table
##Limiting to the experiments that have been done
superDict = Dict(s => unique(linesToType[(linesToType[:Supertype].==s) .& 
        [td in uniqueTypesUsed for td in linesToType[:Type_Description]],:Type_Description]) for 
    s in supertypes)
superDict = filter((k,x) -> !isempty(x),superDict)

writeJS("../supertypes.js","SUPERTYPES",superDict)

summaryDataDict = 
Dict(cp => Dict(nP => Dict(string(n) => 
stats_per_pair[(stats_per_pair[:cellPair].==cp) .& (stats_per_pair[:nPulses_median].==nP),
    n][1] for 
    n in names(stats_per_pair)[[2,3,collect(5:end)...]]) for
    nP in stats_per_pair[stats_per_pair[:cellPair].==cp,:nPulses_median]) for 
    cp in unique(stats_per_pair[:cellPair]))
            ## Only exporting the ones that are in the full set (excludes low ns)



## PAIRS REQUIRING OUR ATTENTION FOR REPEATS
#unique(stats_per_pair[stats_per_pair[:n].<=3,:cellPair])
superSummary = 
Dict(cp => Dict(string(n) => 
stats_per_pair_20[(stats_per_pair_20[:cellPair].==cp),n][1] for 
            n in [:n,:dose_peak_norm,:dose_slope_peak_norm,:expType,:signif1,:signif5,:distanceNorm]) for
            cp in unique(stats_per_pair_20[:cellPair]))

drugSummary = 
Dict(cp => Dict(string(n) => 
drugStatsDFPerPair[(drugStatsDFPerPair[:cellPair].==cp),n][1] for 
                n in names(drugStatsDFPerPair)) for
            cp in unique(drugStatsDFPerPair[:cellPair]))
## For now we're exporting the stats for the drug free runs
perRunDataDict = 
Dict(cp => Dict(nP => Dict(string(n) => 
stats_per_run[(stats_per_run[:cellPair].==cp) .& (stats_per_run[:nPulses_median].==nP) .& (stats_per_run[:preDrug]),
    n] for 
                n in names(stats_per_run)[[collect(3:end)...]]) for
    nP in stats_per_run[stats_per_run[:cellPair].==cp,:nPulses_median]) for 
    cp in unique(stats_per_run[:cellPair]))

## A table of drivers, used by the website
drivers = Dict(td => convert(Array,
        linesToType[linesToType[:Type_Description].== td,:Line]) for 
                          td in unique(linesToType[:Type_Description]));
writeJS("../drivers.js","DRIVERS",drivers)

writeJS("../perRunData.js","PER_RUN_DATA",perRunDataDict)
writeJS("../neurontypes.js","NEURON_TYPES",neuronTypes)
writeJS("../summaryData.js","SUMMARY_DATA",summaryDataDict)
writeJS("../superSummary.js","SUPER_SUMMARY",superSummary)
writeJS("../drugSummary.js","DRUG_SUMMARY",drugSummary)
