function interpolate_run(run,time_axis)
    itp = interpolate((run[1][Axis{:time}][:],),vec(run[1]),Gridded(Linear()))
    (AxisArray(itp[time_axis],Axis{:time}(time_axis)),run[2])
end

## A function that returns the stats per repeats (the input is an array of runs generated by the raw analysis script) MOVE TO FUNC CONN PACKAGE ?
function getStatistics(runArray)
    nRuns = length(runArray)
    periods = [x[1][Axis{:time}][2] for x in runArray]
    nPulses = [x[2]["pulseNumber"] for x in runArray]
    
    resultsDf = DataFrame(Array{Number,1}[Array{Float64,1}(),
                         Array{Float64,1}(),
                         Array{Float64,1}(),
                         Array{Float64,1}(),
                         Array{Float64,1}(),
                         Array{Float64,1}(),
                         Array{Float64,1}(),
                         Array{Float64,1}(),
                         Array{Float64,1}(),
                         Array{Int,1}(),
                         Array{Float64,1}()],
                           [:baseline,
                            :integrals,
                            :integral_to_peak,
                            :integNorm,
                            :peakFluo,
                            :peakTimes,
                            :peakNorm,
                            :repeats_correlation,
                            :half_decay,
                            :nPulses,
                            :runIdx])
    
    for tt in 1:nRuns
        run = squeeze(runArray[tt][1],2)
        ## The times we consider to be "stim" (goes a bit after the stim to account for GC6m rising time and CsChrimson dynamics, important for very short stimulations)
        stimTimes = 3..(3+max(nPulses[tt]/30,periods[tt]))
        
        ## Times used for the calculation of baselines
        preTimes = 1.9..2.9
        ## Second after stim
        postTimes = (3+nPulses[tt]/30)..(4+nPulses[tt]/30)
        fullTimes = 3..(4+nPulses[tt]/30)
        
        baseline = mean(run[preTimes,:],1)
        
        fluoStim = run[stimTimes,:]
        stimStart = fluoStim[Axis{:time}][1]
        fluoFull = run[fullTimes,:]
        
        integrals = vec(sum(fluoStim.-baseline,1))*periods[tt]
        # We want to look at the peak that has the same sign as the integral                    
        extremaFunction = [integrals[i] >= 0 ? maximum : minimum for 
            i in eachindex(integrals)]  
        findFunction = [integrals[i] >= 0 ? indmax : indmin for i in eachindex(integrals)]
            
        peakFluo = vec([extremaFunction[i](fluoFull[:,i]-baseline[i],1)[1] 
                for i in eachindex(extremaFunction)]       )
        peakTimes = [fluoFull[Axis{:time}][findFunction[i](fluoFull[:,i])]-stimStart 
            for i in eachindex(peakFluo)]
        
        ## Find the half-decay time (considering baseline as the value it's coming back to)
        halfDecay = [findfirst(abs.(run[(peakTimes[i]+stimStart)..Inf,i]-baseline[i]).<abs(peakFluo[i]/2))*periods[tt] for i in eachindex(peakTimes)]
        
        ## Integral to the peak of the response
        integralToPeak = vec([sum(run[3..(peakTimes[i]+stimStart),i]-baseline[i],
                    1)[1]*periods[tt] for i in eachindex(peakTimes)])
        
        repeatsCor = repeat([(sum(cor(run[3..3.66,:],1))-4)/12],inner=[4])  
        ## Getting the mean correlation during the 
        ## stimulus during the 4 repeats as a measure 
        ## of reliability (it's the same value repeated here)

        ## Measurements normalized by baseline
        peakFluoNorm = peakFluo./vec(baseline)
        integralToPeakNorm = (integralToPeak - nPulses[tt]*0.0005)./vec(baseline) ## Small ad-hoc correction to avoid influence of the stim artefact
        
        miniDf = DataFrame(Array{Number,1}[vec(baseline),
                               integrals,
                               integralToPeak,
                               integralToPeakNorm,
                               peakFluo,peakTimes,peakFluoNorm,
                               repeatsCor,halfDecay,
                               repeat([nPulses[tt]],inner=[4]),
                               repeat([tt],inner=[4])],
                           [:baseline,
                            :integrals,
                            :integral_to_peak,
                            :integNorm,
                            :peakFluo,
                            :peakTimes,
                            :peakNorm,
                            :repeats_correlation,
                            :half_decay,
                            :nPulses,
                            :runIdx])
        
        resultsDf = vcat(resultsDf,miniDf)
    end
    resultsDf
end

## Run the stats computation on a dict of runs and concatenate them in a single data frame
function compileStats(runArrayDict,keys)
    statsDf = DataFrame([Array{Float64,1}(),
                         Array{Float64,1}(),
                         Array{Float64,1}(),
                         Array{Float64,1}(),
                         Array{Float64,1}(),
                         Array{Float64,1}(),
                         Array{Float64,1}(),
                         Array{Float64,1}(),
                         Array{Float64,1}(),
                         Array{Int,1}(),
    Array{Float64,1}(),
    Array{String,1}()],
                           [:baseline,
                            :integrals,
                            :integral_to_peak,
                            :integNorm,
                            :peakFluo,
                            :peakTimes,
                            :peakNorm,
                            :repeats_correlation,
                            :half_decay,
                            :nPulses,
                            :runIdx,
                            :experiment])
    for (k,runArray) in runArrayDict
        if (k in keys)
            runDf = getStatistics(runArray)
            runDf[:experiment] = k
            statsDf = vcat(statsDf,runDf)
        end
    end
   
    statsDf
end

## A function that removes outliers to get a better estimate of the covariance matrix for
## distance evaluation TO COMMIT TO SOME STAT PACKAGE
function fastMCD(X,p = ceil(Int,(sum(size(X))+1)/2);nrepeats=500)
    rng = Base.GLOBAL_RNG
    hMin = nothing
    sMin = Inf
  
    for i in 1:nrepeats
        idx = randperm(size(X,1))
        h1 = X[idx[1:p],:]    
        s0 = 0
        s1= 1
        while ((det(s1)!=det(s0)) & (det(s1)!=0))
            h0 = h1
            s0 = cov(h0)
            m = vec(mean(h0,1))
            Dis = vec(mapslices(x -> mahalanobis(x,m,inv(s0)),X,2))
            ord = sortperm(Dis)
            h1 = X[ord[1:p],:]
            s1=cov(h1)
       end
       
        if det(s1)<det(sMin)
            hMin = h1
            sMin = s1
        end
    end
    ## Reweighting
    sfull = cov(hMin)
    tmcd = vec(mean(hMin,1))
    dfull = vec(mapslices(x -> mahalanobis(x,tmcd,inv(sfull)),hMin,2))
    smcd = (median(dfull.^2)/pdf(Chisq(size(X,2)),0.5))*sfull
    dmcd = vec(mapslices(x -> mahalanobis(x,tmcd,inv(smcd)),hMin,2));
    w = FrequencyWeights(((dmcd.^2).<pdf(Chisq(size(X,2)),0.975))*1)
    t1 = mean(hMin,w,1)
    s1 = cov(hMin,w,corrected=true)
    (t1,s1)
end

    ## Per run distance evaluation, evaluated separately for each stim condition
function addDistances!(statDf,stats_to_use,signVariable)
    statDf[:signif1]=false
    statDf[:signif5]=false
    statDf[:distance]=0.0
    for p in [1,5,10,20,30]
        locDF = statDf[statDf[:nPulses_median].==p,:]
        null_sample = locDF[locDF[:expType].=="Non overlapping",stats_to_use]
        ## Robust mean and covariance of the control sample
        (null_mean,null_cov) = fastMCD(convert(Array,null_sample))
    
        ## Distance evaluation
        dM = mapslices(x-> mahalanobis(x,vec(null_mean),inv(null_cov)),
                       convert(Array,locDF[:,stats_to_use]),2)
        
        ## Use the null distribution of distances to evaluate the confidence intervals
        null_dist = dM[locDF[:expType].=="Non overlapping"]
        centerB = bootstrap(null_dist,mean,BalancedSampling(1000))
        s1 = Bootstrap.ci(centerB,BCaConfInt(0.98))[1][3]
        s5 = Bootstrap.ci(centerB,BCaConfInt(0.9))[1][3]
   
        ## Significance calculation, update the table
        statDf[statDf[:nPulses_median].==p,:signif1] = dM.> s1 ;
        statDf[statDf[:nPulses_median].==p,:signif5] = dM.> s5 ;
        statDf[statDf[:nPulses_median].==p,:distance] = dM .* sign.(statDf[statDf[:nPulses_median].==p,
                                                                           signVariable]);
    end
    statDf
end

## A function to aggregate stats for a group of runs (for example for a cell pair) TO MOVE TO FUNC CONN PACKAGE ?
function category_stats(df)
    ## Calculate the between run correlations for a given pair/genotype
    avg_data = vcat([interpolated_data_dict[exp][df[df[:experiment].==exp,:runIdx]] for 
                exp in unique(df[:experiment])]...)
    cc = hcat([a[1] for a in avg_data]...)
    cc = cor(cc,1)
     
    ## THE NORMALIZED VARIABLES ARE CRUDE ESTIMATES, MIGHT BE WORTH SWITCHING TO SOME ROBUST REGRESSION WHEN IT'S MATURE IN JULIA
    
    DataFrame(peakFluo = median(df[:peakFluo_median]),
              peakNorm = median(df[:peakNorm_median]),#median(df[:peakFluo_median]./df[:baseline_median]),#predi["peak"][1],#a + b,#itp[0.3],
              peakNormMAD = mad(df[:peakNorm_median]),#mad(df[:peakFluo_median]./df[:baseline_median]),
              integ = median(df[:integral_to_peak_median]),
              integNorm = median(df[df[:baseline_median].>0.2,:integNorm_median]),#Ignoring very low baseline runs #median(df[:integral_to_peak_median]./df[:baseline_median]),#predi["integral"][1],#a + b,#itp[0.3],
    integNormMAD = mad(df[:integNorm_median]),#mad(df[:integral_to_peak_median]./df[:baseline_median]),
    baseline = median(df[:baseline_median]),
        repeats_corr = median(df[:repeats_correlation_median]),
        peakTime = median(df[:peakTimes_median]),
    state_dependence = cor(df[:distance],df[:baseline_median]),
     state_dependence_peak = cor(df[:peakFluo_median],df[:baseline_median]),
        state_dependence_integral = cor(df[:integral_to_peak_median],df[:baseline_median]),
        between_runs_corr = (sum(cc)-size(cc,1))/(length(cc) - size(cc,1)),
        run_distance = median(df[:distance]),
        preNeuron = df[:preNeuron][1],
        postNeuron = df[:postNeuron][1],
        expType = df[:expType][1],
        decay_time = median(df[:half_decay_median]),
        n = size(df,1)
        ) 
end
